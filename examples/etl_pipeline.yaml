# ETL Pipeline Example
# A typical data warehouse ETL workflow that runs hourly
#
# Flow: extract -> transform -> load -> notify
#
scheduler:
  log_level: info

tasks:
  # Extract data from multiple sources
  - id: extract_mysql
    name: Extract from MySQL
    command: |
      mysqldump -h db.example.com -u etl_user -p$MYSQL_PASSWORD \
        --single-transaction sales_db orders customers \
        > /data/staging/mysql_$(date +%Y%m%d_%H).sql
    cron: "0 * * * *"  # Every hour
    timeout: 1800

  - id: extract_api
    name: Extract from REST API
    command: |
      curl -s -H "Authorization: Bearer $API_TOKEN" \
        "https://api.example.com/v1/transactions?since=$(date -d '1 hour ago' +%s)" \
        > /data/staging/api_$(date +%Y%m%d_%H).json
    cron: "0 * * * *"
    timeout: 600

  # Transform and clean data
  - id: transform
    name: Transform Data
    command: |
      python3 /opt/etl/transform.py \
        --input /data/staging \
        --output /data/processed \
        --date $(date +%Y%m%d_%H)
    deps: [extract_mysql, extract_api]
    timeout: 3600

  # Load to data warehouse
  - id: load_warehouse
    name: Load to Data Warehouse
    command: |
      psql -h warehouse.example.com -U etl_user -d analytics \
        -f /opt/etl/load_script.sql \
        -v processed_dir=/data/processed
    deps: [transform]
    timeout: 1800

  # Send notification
  - id: notify
    name: Send Completion Notification
    command: |
      curl -X POST "https://hooks.slack.com/services/$SLACK_WEBHOOK" \
        -H "Content-Type: application/json" \
        -d '{"text":"ETL pipeline completed successfully at '$(date)'"}'
    deps: [load_warehouse]
    timeout: 60
